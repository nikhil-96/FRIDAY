{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ppo_clip.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install ray --quiet\n",
        "%pip install lz4 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUD4z2lufw_U",
        "outputId": "863978fd-41fb-4c2f-89c2-5981601fad77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 54.5 MB 1.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 41.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 69.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 34.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 461 kB 59.1 MB/s \n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lz4\n",
            "  Downloading lz4-4.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 9.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: lz4\n",
            "Successfully installed lz4-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rYDWmjZqeVB",
        "outputId": "ed0feaf8-0c38-4dd9-bfcd-20830c38f626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a shortcut to shared folder and run this\n",
        "%cd /content/drive/MyDrive/A3_PPO"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghAOOq08kvRH",
        "outputId": "7c70ffb9-6319-4e4d-a060-7b9de815a0bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/A3_PPO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOQ8ZcCzfgVU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import ray\n",
        "import ray.rllib.agents.ppo as ppo\n",
        "# from ray.tune.logger import pretty_print\n",
        "import matplotlib.pyplot as plt\n",
        "#\n",
        "from main.continuous import FridayCleaning\n",
        "from main.robot import Robot\n",
        "from main.parser import parse_config, get_area"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use this code in Chrome terminal (Ctrl+shift+i) to prevent session timeout :**\n",
        "\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click();\n",
        "}\n",
        "var clicker = setInterval(ClickConnect,60000);\n",
        "\n",
        "**And the following code when you need to stop:**\n",
        "\n",
        "clearInterval(clicker);"
      ],
      "metadata": {
        "id": "hB6FLSQG-bHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parent_path = Path(\".\").resolve().parent\n",
        "grid = parse_config(\"grids/house.grid\")\n",
        "robot = Robot(init_position=(0, 8))\n",
        "ray.shutdown()\n",
        "ray.init()\n",
        "config = ppo.DEFAULT_CONFIG.copy()\n",
        "config[\"num_gpus\"] = 1\n",
        "config[\"num_workers\"] = 2\n",
        "config[\"env_config\"] = {\"robot\": robot, \"grid\": grid}\n",
        "config[\"recreate_failed_workers\"] = True\n",
        "config[\"gamma\"] = 0.99\n",
        "config[\"lr\"] = 0.0001\n",
        "config[\"clip_param\"] = 0.2\n",
        "\n",
        "PPO_trainer = ppo.PPOTrainer(env=FridayCleaning, config=config)\n",
        "\n",
        "checkpoint_path = f\"checkpoints/checkpoint{str(config['lr']).replace('.','')}_{str(config['gamma']).replace('.','')}\"\n",
        "train_losses = []\n",
        "train_rewards = []\n",
        "# trainer.train()\n",
        "for i in tqdm(range(200)):\n",
        "    # Perform one iteration of training the policy with PPO\n",
        "    result = PPO_trainer.train()\n",
        "    # print(result))\n",
        "    train_rewards.append(result[\"episode_reward_mean\"])\n",
        "    train_losses.append(result[\"info\"][\"learner\"][\"default_policy\"][\"learner_stats\"][\"total_loss\"])\n",
        "\n",
        "checkpoint_path = PPO_trainer.save(checkpoint_dir=checkpoint_path)\n",
        "print(\"checkpoint saved at\", checkpoint_path)\n",
        "plt.xlabel('train_epoch')\n",
        "plt.ylabel('rewards')\n",
        "plt.plot(train_rewards, label='Rewards')\n",
        "plt.title(f\"Rewards vs Epoch During Training (lr={config['lr']}, gamma={config['gamma']})\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.xlabel('train_epoch')\n",
        "plt.ylabel('losses')\n",
        "plt.plot(train_losses, label='Losses')\n",
        "plt.title(f\"Losses vs Epoch During Training (lr={config['lr']}, gamma={config['gamma']})\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "np.savetxt(f\"train_rewards{str(config['lr']).replace('.','')}_{str(config['gamma']).replace('.','')}.csv\", \n",
        "           train_rewards,\n",
        "           delimiter =\", \", \n",
        "           fmt ='% s')\n",
        "\n",
        "np.savetxt(f\"train_losses{str(config['lr']).replace('.','')}_{str(config['gamma']).replace('.','')}.csv\", \n",
        "           train_losses,\n",
        "           delimiter =\", \", \n",
        "           fmt ='% s')\n",
        "\n",
        "#PPO_trainer.restore(f'{checkpoint_path}/checkpoint_000300/checkpoint-300')\n",
        "PPO_trainer.restore(checkpoint_path)\n",
        "\n",
        "test_cleaning = []\n",
        "test_reward = []\n",
        "\n",
        "for epoch in tqdm(range(10)):\n",
        "    env = FridayCleaning(dict(robot=robot, grid=grid))\n",
        "    obs = env.reset()\n",
        "    #env.render()\n",
        "    running_reward = 0.0\n",
        "    initial_dust_area = sum([get_area(patch) for patch in env.grid.dirt_places]) + \\\n",
        "                        sum([get_area(patch) for patch in env.grid.much_dirt_places]) + sum([get_area(patch) for patch in env.grid.reg_dirt])\n",
        "    s = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        move = PPO_trainer.compute_action(obs)\n",
        "        obs, reward, done, info = env.step(move)\n",
        "        #env.render()\n",
        "        #print(f\"move: {move/(2*np.pi)*360}, reward: {reward}\")\n",
        "        running_reward += reward\n",
        "        s += 1\n",
        "\n",
        "    final_dust_area = sum([get_area(patch) for patch in env.grid.dirt_places]) + \\\n",
        "                      sum([get_area(patch) for patch in env.grid.much_dirt_places]) + sum([get_area(patch) for patch in env.grid.reg_dirt])\n",
        "\n",
        "    cleaning_area = initial_dust_area - final_dust_area\n",
        "    cleaning_per = (cleaning_area / initial_dust_area) * 100\n",
        "\n",
        "    test_reward.append(running_reward)\n",
        "    test_cleaning.append(cleaning_per)\n",
        "    print(f\"Epoch: {epoch} -- Reward: {running_reward} -- Cleaning %: {cleaning_per}\")\n",
        "\n",
        "print(test_reward)\n",
        "print(test_cleaning)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIyR6YxlfkZY",
        "outputId": "aac98c73-54b4-498e-98a9-2cf802c9060b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-06-21 21:20:22,931\tINFO logger.py:188 -- pip install \"ray[tune]\" to see TensorBoard files.\n",
            "2022-06-21 21:20:22,933\tWARNING logger.py:337 -- Could not instantiate TBXLogger: No module named 'tensorboardX'.\n",
            "2022-06-21 21:20:22,965\tINFO trainer.py:2333 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
            "2022-06-21 21:20:22,969\tINFO ppo.py:415 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            "2022-06-21 21:20:22,970\tINFO trainer.py:906 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=603)\u001b[0m 2022-06-21 21:20:29,526\tWARNING env.py:136 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=606)\u001b[0m 2022-06-21 21:20:29,545\tWARNING env.py:136 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
            "2022-06-21 21:20:40,196\tINFO trainable.py:163 -- Trainable.setup took 17.261 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
            "2022-06-21 21:20:40,200\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
            " 60%|██████    | 120/200 [21:56<15:00, 11.25s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Qki2kIWfomkn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}